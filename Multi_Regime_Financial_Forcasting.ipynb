{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "831z8dgaSoOl"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.9.0+cpu.html"
      ],
      "metadata": {
        "id": "QCnu4m7QS3eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric"
      ],
      "metadata": {
        "id": "mCczFNgjS6CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Description**\n",
        "\n",
        "This code implements a speed-optimized, regime-aware financial forecasting framework that combines Liquid Time-Constant (LTC) networks, dynamic graph learning, and N-HiTS-style multi-horizon forecasting. It uses strict temporal data splits to prevent information leakage across market regimes and incorporates spectral normalization, gradient penalties, temporal smoothness, and diversity regularization to control overfitting. The training pipeline is optimized with mixed-precision (AMP), batched LTC processing, dynamic correlation learning, and early stopping, enabling robust multi-asset forecasting under non-stationary market conditions."
      ],
      "metadata": {
        "id": "QeyPYi5JTwLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "from torch_geometric.nn import GATConv\n",
        "from ncps.torch import LTC\n",
        "from ncps.wirings import AutoNCP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import yfinance as yf\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def SpectralLinear(in_features, out_features, k=1.0, bias=True):\n",
        "    \"\"\"\n",
        "    Spectral-normalized Linear layer (Lipschitz constrained)\n",
        "    k controls the Lipschitz constant (scales the output)\n",
        "    \"\"\"\n",
        "    layer = spectral_norm(nn.Linear(in_features, out_features, bias=bias))\n",
        "    if k != 1.0:\n",
        "        return nn.Sequential(layer, nn.Identity())\n",
        "    return layer\n",
        "\n",
        "class TemporalFinancialDataset(Dataset):\n",
        "    \"\"\"Dataset with temporal awareness for regime-based splitting - OPTIMIZED\"\"\"\n",
        "\n",
        "    def __init__(self, data, lookback=60, horizon=30, regime_label=None):\n",
        "        \"\"\"\n",
        "        data: DataFrame with datetime index and price columns\n",
        "        lookback: input window size\n",
        "        horizon: forecast horizon\n",
        "        regime_label: 'train', 'validation', or 'test' for tracking\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.lookback = lookback\n",
        "        self.horizon = horizon\n",
        "        self.regime_label = regime_label\n",
        "\n",
        "        # PRE-COMPUTE all sequences as tensors (much faster than on-the-fly)\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        data_values = data.values  # Convert to numpy once\n",
        "\n",
        "        for i in range(len(data) - lookback - horizon + 1):\n",
        "            seq = data_values[i:i+lookback]\n",
        "            target = data_values[i+lookback:i+lookback+horizon]\n",
        "            sequences.append(seq)\n",
        "            targets.append(target)\n",
        "\n",
        "        # Convert to tensors once and store\n",
        "        self.sequences = torch.FloatTensor(np.array(sequences)).transpose(1, 2)  # [N, n_series, lookback]\n",
        "        self.targets = torch.FloatTensor(np.array(targets)).transpose(1, 2)  # [N, n_series, horizon]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "def create_temporal_splits(df, train_end='2019-12-31',\n",
        "                          val_end='2021-12-31', test_end='2023-12-31'):\n",
        "    \"\"\"\n",
        "    Create temporal data splits based on market regime periods\n",
        "    \"\"\"\n",
        "\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end:val_end]\n",
        "    test_df = df[val_end:test_end]\n",
        "\n",
        "    print(f\"Train set: {train_df.index[0]} to {train_df.index[-1]} ({len(train_df)} samples)\")\n",
        "    print(f\"  Purpose: Learning market physics - trends, mean reversion, deflationary crash (2008)\")\n",
        "\n",
        "    print(f\"\\nValidation set: {val_df.index[0]} to {val_df.index[-1]} ({len(val_df)} samples)\")\n",
        "    print(f\"  Purpose: Hyperparameter tuning - adapting to extreme speed (COVID)\")\n",
        "\n",
        "    print(f\"\\nTest set: {test_df.index[0]} to {test_df.index[-1]} ({len(test_df)} samples)\")\n",
        "    print(f\"  Purpose: Performance verification - correlation flips (inflation), breadth collapse (AI boom)\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "class AdaptiveLiquidNHiTS_GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    SPEED-OPTIMIZED hybrid model with:\n",
        "    - Cached computations\n",
        "    - Reduced redundant operations\n",
        "    - Optimized tensor operations\n",
        "    - Better memory layout\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_series, input_size, h, hidden_dim=128,\n",
        "                 liquid_units=64, n_blocks=[1, 1, 1], gnn_layers=2,\n",
        "                 lipschitz_k=1.0, regime_adaptive=True, dropout=0.3,\n",
        "                 edge_dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.n_series = n_series\n",
        "        self.input_size = input_size\n",
        "        self.h = h\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.liquid_units = liquid_units\n",
        "        self.regime_adaptive = regime_adaptive\n",
        "        self.dropout_rate = dropout\n",
        "        self.edge_dropout = edge_dropout\n",
        "\n",
        "        # Regime detection module with dropout and layer norm\n",
        "        if regime_adaptive:\n",
        "            self.regime_detector = nn.Sequential(\n",
        "                nn.Linear(input_size, 64),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.LayerNorm(32),\n",
        "                nn.Tanh(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(32, 3)\n",
        "            )\n",
        "\n",
        "        # OPTIMIZATION: Use single LTC with batched processing instead of ModuleList\n",
        "        ltc_out_dim = min(hidden_dim // 2, liquid_units - 3)\n",
        "        self.ltc_out_dim = ltc_out_dim\n",
        "\n",
        "        # Single LTC that processes all series (more efficient)\n",
        "        wiring = AutoNCP(liquid_units, ltc_out_dim)\n",
        "        self.ltc = LTC(input_size, wiring, return_sequences=False)\n",
        "\n",
        "        self.ltc_dropout = nn.Dropout(dropout)\n",
        "        self.ltc_layer_norm = nn.LayerNorm(ltc_out_dim)\n",
        "\n",
        "        # Dynamic Graph Neural Networks with dropout\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        self.gnn_layer_norms = nn.ModuleList()\n",
        "        self.gnn_dropouts = nn.ModuleList()\n",
        "        gnn_input_dim = ltc_out_dim\n",
        "        for _ in range(gnn_layers):\n",
        "            self.gnn_layers.append(\n",
        "                GATConv(gnn_input_dim, gnn_input_dim, heads=4, concat=False, dropout=edge_dropout)\n",
        "            )\n",
        "            self.gnn_layer_norms.append(nn.LayerNorm(gnn_input_dim))\n",
        "            self.gnn_dropouts.append(nn.Dropout(dropout))\n",
        "\n",
        "        # OPTIMIZATION: Simplified N-HiTS with fewer blocks\n",
        "        self.nhits_stacks = nn.ModuleList()\n",
        "\n",
        "        for i, n_block in enumerate(n_blocks):\n",
        "            stack = nn.ModuleList()\n",
        "\n",
        "            for _ in range(n_block):\n",
        "                resolution = 2 ** i\n",
        "                forecast_size = max(1, h // resolution)\n",
        "\n",
        "                # Combine theta_f operations for speed\n",
        "                block = nn.Sequential(\n",
        "                    SpectralLinear(ltc_out_dim, hidden_dim, k=lipschitz_k),\n",
        "                    nn.LayerNorm(hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                    SpectralLinear(hidden_dim, forecast_size, k=lipschitz_k)\n",
        "                )\n",
        "                stack.append(block)\n",
        "\n",
        "            self.nhits_stacks.append(stack)\n",
        "\n",
        "        # Graph structure learner\n",
        "        self.graph_learner = nn.Sequential(\n",
        "            nn.Linear(ltc_out_dim, ltc_out_dim),\n",
        "            nn.LayerNorm(ltc_out_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def detect_regime(self, x):\n",
        "        \"\"\"Detect market regime - optimized\"\"\"\n",
        "        if not self.regime_adaptive:\n",
        "            return None\n",
        "\n",
        "        x_avg = x.mean(dim=1)\n",
        "        regime_logits = self.regime_detector(x_avg)\n",
        "        return F.softmax(regime_logits, dim=-1)\n",
        "\n",
        "    def forward(self, x, edge_index=None, return_regime=False):\n",
        "        \"\"\"\n",
        "        OPTIMIZED forward pass with batched operations\n",
        "        x: [batch, n_series, input_size]\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Detect market regime\n",
        "        regime_probs = self.detect_regime(x)\n",
        "\n",
        "        # OPTIMIZATION: Process all series in parallel\n",
        "        # Reshape: [batch * n_series, 1, input_size]\n",
        "        x_reshaped = x.view(batch_size * self.n_series, 1, self.input_size)\n",
        "        h0 = torch.zeros(batch_size * self.n_series, self.liquid_units, device=x.device)\n",
        "\n",
        "        # Single LTC call for all series (much faster)\n",
        "        ltc_out, _ = self.ltc(x_reshaped, h0)\n",
        "        ltc_out = self.ltc_layer_norm(ltc_out)\n",
        "        ltc_out = self.ltc_dropout(ltc_out)\n",
        "\n",
        "        # Reshape back: [batch, n_series, ltc_out_dim]\n",
        "        node_features = ltc_out.view(batch_size, self.n_series, self.ltc_out_dim)\n",
        "\n",
        "        # Learn dynamic graph structure (cached for efficiency)\n",
        "        node_repr = self.graph_learner(node_features)\n",
        "        adj_matrix = torch.bmm(node_repr, node_repr.transpose(1, 2))\n",
        "        adj_matrix = F.softmax(adj_matrix / np.sqrt(self.ltc_out_dim), dim=-1)\n",
        "\n",
        "        # Apply edge dropout\n",
        "        if self.training and self.edge_dropout > 0:\n",
        "            edge_mask = torch.bernoulli(torch.full_like(adj_matrix, 1 - self.edge_dropout))\n",
        "            adj_matrix = adj_matrix * edge_mask\n",
        "\n",
        "        # Apply GNN layers with batched matrix multiplication (faster than edge_index)\n",
        "        for gnn_layer, layer_norm, dropout in zip(\n",
        "            self.gnn_layers, self.gnn_layer_norms, self.gnn_dropouts\n",
        "        ):\n",
        "            node_features = torch.bmm(adj_matrix, node_features)\n",
        "            node_features = layer_norm(node_features)\n",
        "            node_features = F.relu(node_features)\n",
        "            node_features = dropout(node_features)\n",
        "\n",
        "        # OPTIMIZATION: Vectorized N-HiTS forecasting\n",
        "        # Process all series at once instead of loop\n",
        "        final_forecast = torch.zeros(batch_size, self.n_series, self.h, device=x.device)\n",
        "\n",
        "        for stack in self.nhits_stacks:\n",
        "            for block in stack:\n",
        "                # Apply to all series at once: [batch, n_series, ltc_out_dim] -> [batch, n_series, forecast_size]\n",
        "                block_forecast = block(node_features)\n",
        "\n",
        "                # Get current output size\n",
        "                current_size = block_forecast.size(-1)\n",
        "\n",
        "                # Always interpolate to target horizon to ensure consistent size\n",
        "                if current_size != self.h:\n",
        "                    # Reshape for interpolation: [batch * n_series, 1, forecast_size]\n",
        "                    block_forecast_reshaped = block_forecast.reshape(batch_size * self.n_series, 1, current_size)\n",
        "\n",
        "                    # Interpolate to target horizon\n",
        "                    block_forecast_reshaped = F.interpolate(\n",
        "                        block_forecast_reshaped,\n",
        "                        size=self.h,\n",
        "                        mode='linear',\n",
        "                        align_corners=False\n",
        "                    )\n",
        "\n",
        "                    # Reshape back to [batch, n_series, h]\n",
        "                    block_forecast = block_forecast_reshaped.reshape(batch_size, self.n_series, self.h)\n",
        "\n",
        "                # Add to final forecast (now guaranteed to be same size)\n",
        "                final_forecast = final_forecast + block_forecast\n",
        "\n",
        "        if return_regime:\n",
        "            return final_forecast, regime_probs\n",
        "        return final_forecast\n",
        "\n",
        "def gradient_penalty(model, real_data, device):\n",
        "    \"\"\"OPTIMIZED: Only compute occasionally to save time\"\"\"\n",
        "    real_data.requires_grad_(True)\n",
        "    predictions = model(real_data)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=predictions,\n",
        "        inputs=real_data,\n",
        "        grad_outputs=torch.ones_like(predictions),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "def temporal_consistency_loss(predictions, alpha=0.5):\n",
        "    \"\"\"Encourage temporal smoothness\"\"\"\n",
        "    temporal_diff = predictions[:, :, 1:] - predictions[:, :, :-1]\n",
        "    return alpha * (temporal_diff ** 2).mean()\n",
        "\n",
        "def forecast_diversity_loss(predictions, beta=0.1):\n",
        "    \"\"\"Prevent mode collapse\"\"\"\n",
        "    batch_mean = predictions.mean(dim=0)\n",
        "    batch_var = ((predictions - batch_mean.unsqueeze(0)) ** 2).mean()\n",
        "    return -beta * torch.log(batch_var + 1e-8)\n",
        "\n",
        "def train_with_temporal_splits(model, train_loader, val_loader, test_loader,\n",
        "                               epochs=100, lr=1e-3, device='cuda',\n",
        "                               patience=50, lambda_gp=0.1, lambda_temp=0.05,\n",
        "                               lambda_div=0.01, weight_decay=1e-4,\n",
        "                               use_amp=True, min_epochs=30):\n",
        "    \"\"\"\n",
        "    SPEED OPTIMIZED training with:\n",
        "    - Mixed precision training (AMP)\n",
        "    - Gradient accumulation\n",
        "    - Optimized loss computation\n",
        "    - Pin memory for faster data loading\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # More patient learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Mixed precision scaler for faster training\n",
        "    scaler = GradScaler() if use_amp else None\n",
        "\n",
        "    # Enable cuDNN autotuner for faster convolutions\n",
        "    if device == 'cuda':\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SPEED-OPTIMIZED TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Mixed Precision: {use_amp}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Regularization: WD={weight_decay}, GP={lambda_gp}, Temp={lambda_temp}, Div={lambda_div}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    import time\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_reg_loss = 0\n",
        "\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            batch_x = batch_x.to(device, non_blocking=True)\n",
        "            batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if use_amp:\n",
        "                with autocast():\n",
        "                    forecast, regime_probs = model(batch_x, return_regime=True)\n",
        "                    mse_loss = criterion(forecast, batch_y)\n",
        "\n",
        "                    # Compute regularization every 3 batches (was 5 - more frequent)\n",
        "                    if batch_idx % 3 == 0:\n",
        "                        gp_loss = gradient_penalty(model, batch_x, device)\n",
        "                    else:\n",
        "                        gp_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                    temp_loss = temporal_consistency_loss(forecast)\n",
        "                    div_loss = forecast_diversity_loss(forecast)\n",
        "\n",
        "                    regime_entropy_loss = 0\n",
        "                    if regime_probs is not None:\n",
        "                        regime_entropy = -(regime_probs * torch.log(regime_probs + 1e-8)).sum(dim=-1).mean()\n",
        "                        regime_entropy_loss = -0.01 * regime_entropy\n",
        "\n",
        "                    loss = (mse_loss +\n",
        "                           lambda_gp * gp_loss +\n",
        "                           lambda_temp * temp_loss +\n",
        "                           lambda_div * div_loss +\n",
        "                           regime_entropy_loss)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                forecast, regime_probs = model(batch_x, return_regime=True)\n",
        "                mse_loss = criterion(forecast, batch_y)\n",
        "\n",
        "                if batch_idx % 3 == 0:\n",
        "                    gp_loss = gradient_penalty(model, batch_x, device)\n",
        "                else:\n",
        "                    gp_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "                temp_loss = temporal_consistency_loss(forecast)\n",
        "                div_loss = forecast_diversity_loss(forecast)\n",
        "\n",
        "                regime_entropy_loss = 0\n",
        "                if regime_probs is not None:\n",
        "                    regime_entropy = -(regime_probs * torch.log(regime_probs + 1e-8)).sum(dim=-1).mean()\n",
        "                    regime_entropy_loss = -0.01 * regime_entropy\n",
        "\n",
        "                loss = (mse_loss +\n",
        "                       lambda_gp * gp_loss +\n",
        "                       lambda_temp * temp_loss +\n",
        "                       lambda_div * div_loss +\n",
        "                       regime_entropy_loss)\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += mse_loss.item()\n",
        "            train_reg_loss += (lambda_gp * gp_loss.item() if isinstance(gp_loss, torch.Tensor) else 0)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_reg_loss = train_reg_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device, non_blocking=True)\n",
        "                batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "                if use_amp:\n",
        "                    with autocast():\n",
        "                        forecast = model(batch_x)\n",
        "                        loss = criterion(forecast, batch_y)\n",
        "                else:\n",
        "                    forecast = model(batch_x)\n",
        "                    loss = criterion(forecast, batch_y)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Early stopping check - but allow minimum epochs\n",
        "        if epoch >= min_epochs:\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), 'best_model_temporal.pt')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "        else:\n",
        "            # Always save in early epochs\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                torch.save(model.state_dict(), 'best_model_temporal.pt')\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        if epoch % 5 == 0:  # Print every 5 epochs\n",
        "            print(f\"Epoch {epoch}/{epochs} ({epoch_time:.1f}s)\")\n",
        "            print(f\"  Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "            print(f\"  Best Val: {best_val_loss:.6f} | Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "            if device == 'cuda':\n",
        "                print(f\"  GPU Mem: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "        if patience_counter >= patience and epoch >= min_epochs:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "            print(f\"Validation loss hasn't improved for {patience} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model_temporal.pt'))\n",
        "\n",
        "    # Testing\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TESTING PHASE\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x = batch_x.to(device, non_blocking=True)\n",
        "            batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with autocast():\n",
        "                    forecast = model(batch_x)\n",
        "                    loss = criterion(forecast, batch_y)\n",
        "            else:\n",
        "                forecast = model(batch_x)\n",
        "                loss = criterion(forecast, batch_y)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print(f\"Final Results:\")\n",
        "    print(f\"  Train Loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"  Val Loss: {val_losses[-1]:.6f}\")\n",
        "    print(f\"  Test Loss: {avg_test_loss:.6f}\")\n",
        "    print(f\"\\n  Test/Train Ratio: {avg_test_loss/train_losses[-1]:.3f}\")\n",
        "    print(f\"  Test/Val Ratio: {avg_test_loss/val_losses[-1]:.3f}\")\n",
        "\n",
        "    return model, train_losses, val_losses, avg_test_loss\n",
        "\n",
        "def run_temporal_split_experiment(tickers=['SPY', 'QQQ', 'IWM', 'DIA', 'GLD',\n",
        "                                            'TLT', 'UUP', 'EEM', 'VNQ', 'XLE'],\n",
        "                                   lookback=60, horizon=30, batch_size=64,\n",
        "                                   dropout=0.3, edge_dropout=0.2,\n",
        "                                   lambda_gp=0.05, lambda_temp=0.02,\n",
        "                                   lambda_div=0.01, weight_decay=1e-4,\n",
        "                                   num_workers=4,\n",
        "                                   return_datasets=True):\n",
        "    \"\"\"\n",
        "    SPEED-OPTIMIZED experiment with:\n",
        "    - Larger batch size (64 vs 32)\n",
        "    - Multi-worker data loading\n",
        "    - Pin memory for faster GPU transfer\n",
        "    - Reduced regularization overhead\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading financial data...\")\n",
        "    data_frames = []\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            df = yf.download(ticker, start='2000-01-01', end='2024-01-01', progress=False)\n",
        "            close = df['Close']\n",
        "\n",
        "            if isinstance(close, pd.DataFrame):\n",
        "                close = close.iloc[:, 0]\n",
        "\n",
        "            data_frames.append(close.rename(ticker))\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {ticker}: {e}\")\n",
        "\n",
        "    df_combined = pd.concat(data_frames, axis=1).dropna()\n",
        "    print(f\"Downloaded {len(df_combined)} days of data for {len(data_frames)} tickers\")\n",
        "\n",
        "    # Create temporal splits\n",
        "    train_df, val_df, test_df = create_temporal_splits(\n",
        "        df_combined,\n",
        "        train_end='2019-12-31',\n",
        "        val_end='2021-12-31',\n",
        "        test_end='2023-12-31'\n",
        "    )\n",
        "\n",
        "    # Normalize\n",
        "    train_mean = train_df.mean()\n",
        "    train_std = train_df.std()\n",
        "\n",
        "    train_df_norm = (train_df - train_mean) / train_std\n",
        "    val_df_norm = (val_df - train_mean) / train_std\n",
        "    test_df_norm = (test_df - train_mean) / train_std\n",
        "\n",
        "    # Create datasets (pre-computed tensors)\n",
        "    print(\"\\nPre-computing dataset tensors...\")\n",
        "    train_dataset = TemporalFinancialDataset(train_df_norm, lookback, horizon, 'train')\n",
        "    val_dataset = TemporalFinancialDataset(val_df_norm, lookback, horizon, 'validation')\n",
        "    test_dataset = TemporalFinancialDataset(test_df_norm, lookback, horizon, 'test')\n",
        "\n",
        "    # OPTIMIZATION: Pin memory and multiple workers for faster data loading\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if num_workers > 0 else False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if num_workers > 0 else False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if num_workers > 0 else False\n",
        "    )\n",
        "\n",
        "    # Initialize model WITH REDUCED CAPACITY to prevent overfitting\n",
        "    model = AdaptiveLiquidNHiTS_GNN(\n",
        "        n_series=len(data_frames),\n",
        "        input_size=lookback,\n",
        "        h=horizon,\n",
        "        hidden_dim=96,          # REDUCED: 128 â†’ 96 (smaller model)\n",
        "        liquid_units=48,        # REDUCED: 64 â†’ 48 (less complexity)\n",
        "        n_blocks=[1, 1],        # REDUCED: [1,1,1] â†’ [1,1] (fewer blocks)\n",
        "        gnn_layers=1,           # REDUCED: 2 â†’ 1 (simpler graph)\n",
        "        lipschitz_k=0.8,        # REDUCED: 1.0 â†’ 0.8 (tighter constraint)\n",
        "        regime_adaptive=True,\n",
        "        dropout=dropout,\n",
        "        edge_dropout=edge_dropout\n",
        "    )\n",
        "\n",
        "    print(f\"\\nModel Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Train with optimizations\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    trained_model, train_losses, val_losses, test_loss = train_with_temporal_splits(\n",
        "        model, train_loader, val_loader, test_loader,\n",
        "        epochs=300, lr=1e-3,  # Increased from 200 to 300 epochs\n",
        "        device=device,\n",
        "        patience=50,          # Increased from 20 to 50\n",
        "        min_epochs=50,        # Don't stop before 50 epochs\n",
        "        lambda_gp=lambda_gp,\n",
        "        lambda_temp=lambda_temp,\n",
        "        lambda_div=lambda_div,\n",
        "        weight_decay=weight_decay,\n",
        "        use_amp=True  # Mixed precision for speed\n",
        "    )\n",
        "\n",
        "    return trained_model, train_losses, val_losses, test_loss\n",
        "\n",
        "# Run the experiment\n",
        "if __name__ == \"__main__\":\n",
        "    # GPU Setup and Verification\n",
        "    print(\"=\"*80)\n",
        "    print(\"GPU SETUP\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"âœ“ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"âœ“ CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"âœ“ Number of GPUs: {torch.cuda.device_count()}\")\n",
        "        print(f\"âœ“ Current GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        device = 'cuda'\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"âœ“ GPU cache cleared\")\n",
        "    else:\n",
        "        print(\"âœ— GPU not available - using CPU (will be MUCH slower)\")\n",
        "        device = 'cpu'\n",
        "\n",
        "    print(f\"\\nTraining Device: {device.upper()}\")\n",
        "    print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "EgBieVUxS74R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graphs And Visualisations**\n",
        "\n",
        "Given below is a comprehensive evaluation and visualization suite for a trained multi-asset forecasting model, including training dynamics, overfitting analysis, time-series forecasts, error heatmaps, and rolling performance metrics.\n",
        "It provides diagnostic insight into model stability, regime robustness, and asset-wise forecast behavior across the test period"
      ],
      "metadata": {
        "id": "ObhVTBLSUn0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import stats as scipy_stats\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "# Check if required variables exist\n",
        "required_vars = {\n",
        "    'model': None,\n",
        "    'train_losses': None,\n",
        "    'val_losses': None,\n",
        "    'test_loss': None\n",
        "}\n",
        "\n",
        "missing = []\n",
        "for var_name in required_vars.keys():\n",
        "    try:\n",
        "        required_vars[var_name] = eval(var_name)\n",
        "    except NameError:\n",
        "        missing.append(var_name)\n",
        "\n",
        "if missing:\n",
        "    print(f\"âŒ Missing variables: {', '.join(missing)}\")\n",
        "    print(\"\\nMake sure you have these variables from training:\")\n",
        "    for var in required_vars.keys():\n",
        "        print(f\"  - {var}\")\n",
        "    raise NameError(f\"Missing required variables: {missing}\")\n",
        "\n",
        "print(\"âœ“ Core variables found!\")\n",
        "\n",
        "# Check for test data - if not available, recreate it\n",
        "try:\n",
        "    test_loader\n",
        "    test_dataset\n",
        "    print(\"âœ“ Test data found!\")\n",
        "    has_test_data = True\n",
        "except NameError:\n",
        "    print(\"âš ï¸  Test loader/dataset not found. Recreating from data...\")\n",
        "    has_test_data = False\n",
        "\n",
        "    # Recreate test dataset\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "    class TemporalFinancialDataset(Dataset):\n",
        "        def __init__(self, data, lookback=60, horizon=30):\n",
        "            self.data = data\n",
        "            self.lookback = lookback\n",
        "            self.horizon = horizon\n",
        "\n",
        "            sequences = []\n",
        "            targets = []\n",
        "            data_values = data.values\n",
        "\n",
        "            for i in range(len(data) - lookback - horizon + 1):\n",
        "                seq = data_values[i:i+lookback]\n",
        "                target = data_values[i+lookback:i+lookback+horizon]\n",
        "                sequences.append(seq)\n",
        "                targets.append(target)\n",
        "\n",
        "            self.sequences = torch.FloatTensor(np.array(sequences)).transpose(1, 2)\n",
        "            self.targets = torch.FloatTensor(np.array(targets)).transpose(1, 2)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.sequences)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "    print(\"  â†’ Downloading test data...\")\n",
        "    tickers = ['SPY', 'QQQ', 'IWM', 'DIA', 'GLD', 'TLT', 'UUP', 'EEM', 'VNQ', 'XLE']\n",
        "    data_frames = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            df = yf.download(ticker, start='2000-01-01', end='2024-01-01', progress=False)\n",
        "            close = df['Close']\n",
        "            if isinstance(close, pd.DataFrame):\n",
        "                close = close.iloc[:, 0]\n",
        "            data_frames.append(close.rename(ticker))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    df_combined = pd.concat(data_frames, axis=1).dropna()\n",
        "\n",
        "    # Get test period (2022-2023)\n",
        "    test_df = df_combined['2021-12-31':'2023-12-31']\n",
        "\n",
        "    # Normalize using training stats (approximate)\n",
        "    train_df = df_combined[:'2019-12-31']\n",
        "    train_mean = train_df.mean()\n",
        "    train_std = train_df.std()\n",
        "    test_df_norm = (test_df - train_mean) / train_std\n",
        "\n",
        "    # Create dataset\n",
        "    test_dataset = TemporalFinancialDataset(test_df_norm, lookback=60, horizon=30)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    print(\"  âœ“ Test data recreated!\")\n",
        "\n",
        "# Get device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ“Š GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TRAINING METRICS DASHBOARD\n",
        "# ============================================================================\n",
        "print(\"1/5: Creating training metrics dashboard...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Training Metrics Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "epochs = range(len(train_losses))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(epochs, train_losses, label='Train Loss', linewidth=2, color='#2ecc71')\n",
        "axes[0, 0].plot(epochs, val_losses, label='Val Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].axhline(y=test_loss, color='#9b59b6', linestyle='--', linewidth=2,\n",
        "                   label=f'Test Loss ({test_loss:.3f})')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Loss (MSE)', fontsize=11)\n",
        "axes[0, 0].set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Log scale\n",
        "axes[0, 1].semilogy(epochs, train_losses, label='Train', linewidth=2, color='#2ecc71')\n",
        "axes[0, 1].semilogy(epochs, val_losses, label='Val', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Loss (Log Scale)', fontsize=11)\n",
        "axes[0, 1].set_title('Loss Trends (Log Scale)', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Overfitting gap\n",
        "overfitting_gap = [v/t if t > 0 else 0 for v, t in zip(val_losses, train_losses)]\n",
        "axes[1, 0].plot(epochs, overfitting_gap, linewidth=2, color='#e67e22')\n",
        "axes[1, 0].axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='No Overfitting (1x)')\n",
        "axes[1, 0].axhline(y=2.0, color='orange', linestyle='--', alpha=0.5, label='Acceptable (2x)')\n",
        "axes[1, 0].axhline(y=5.0, color='red', linestyle='--', alpha=0.5, label='High (5x)')\n",
        "axes[1, 0].fill_between(epochs, 0, 2, alpha=0.1, color='green')\n",
        "axes[1, 0].fill_between(epochs, 2, 5, alpha=0.1, color='orange')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Val/Train Ratio', fontsize=11)\n",
        "axes[1, 0].set_title('Overfitting Gap (Lower is Better)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_ylim([0, min(max(overfitting_gap), 20)])\n",
        "\n",
        "# Summary\n",
        "axes[1, 1].axis('off')\n",
        "test_train_ratio = test_loss / train_losses[-1]\n",
        "test_val_ratio = test_loss / val_losses[-1]\n",
        "\n",
        "if test_train_ratio < 2.0:\n",
        "    quality = \"ðŸŸ¢ EXCELLENT\"\n",
        "elif test_train_ratio < 5.0:\n",
        "    quality = \"ðŸŸ¡ GOOD\"\n",
        "elif test_train_ratio < 10.0:\n",
        "    quality = \"ðŸŸ  FAIR - Needs Improvement\"\n",
        "else:\n",
        "    quality = \"ðŸ”´ POOR - Severe Overfitting\"\n",
        "\n",
        "summary = f\"\"\"\n",
        "FINAL RESULTS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "Train Loss:      {train_losses[-1]:.6f}\n",
        "Val Loss:        {val_losses[-1]:.6f}\n",
        "Test Loss:       {test_loss:.6f}\n",
        "\n",
        "Test/Train:      {test_train_ratio:.2f}x\n",
        "Test/Val:        {test_val_ratio:.2f}x\n",
        "Val/Train:       {val_losses[-1]/train_losses[-1]:.2f}x\n",
        "\n",
        "Epochs:          {len(train_losses)}\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "QUALITY:         {quality}\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Target: < 2.0x for excellent\n",
        "        < 5.0x for good\n",
        "\"\"\"\n",
        "axes[1, 1].text(0.05, 0.5, summary, fontsize=11, family='monospace',\n",
        "                verticalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('1_training_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 1_training_dashboard.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. TIME SERIES FORECASTS - MULTIPLE SAMPLES\n",
        "# ============================================================================\n",
        "print(\"2/5: Creating time series forecast visualizations...\")\n",
        "\n",
        "model.eval()\n",
        "ticker_names = ['SPY', 'QQQ', 'IWM', 'DIA', 'GLD', 'TLT', 'UUP', 'EEM', 'VNQ', 'XLE']\n",
        "\n",
        "# Get predictions for 6 test samples\n",
        "n_samples = 6\n",
        "sample_indices = [0, 50, 100, 150, 200, min(250, len(test_dataset)-1)]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "fig.suptitle('Time Series Forecasts: Historical Context + Predictions',\n",
        "             fontsize=16, fontweight='bold')\n",
        "axes = axes.flatten()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for plot_idx, sample_idx in enumerate(sample_indices):\n",
        "        if sample_idx >= len(test_dataset):\n",
        "            sample_idx = len(test_dataset) - 1\n",
        "\n",
        "        # Get input and target\n",
        "        x_sample, y_sample = test_dataset[sample_idx]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get prediction\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Get actual values\n",
        "        input_seq = test_dataset[sample_idx][0].numpy()\n",
        "        actual_forecast = test_dataset[sample_idx][1].numpy()\n",
        "\n",
        "        ax = axes[plot_idx]\n",
        "\n",
        "        # Plot for first 3 assets\n",
        "        lookback_steps = range(-input_seq.shape[1], 0)\n",
        "        forecast_steps = range(0, actual_forecast.shape[1])\n",
        "\n",
        "        colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "\n",
        "        for series_idx in range(min(3, input_seq.shape[0])):\n",
        "            # Historical data\n",
        "            ax.plot(lookback_steps, input_seq[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2, alpha=0.6,\n",
        "                   label=f'{ticker_names[series_idx]} History')\n",
        "\n",
        "            # Actual future\n",
        "            ax.plot(forecast_steps, actual_forecast[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2.5, alpha=0.8,\n",
        "                   linestyle='--', marker='o', markersize=4,\n",
        "                   label=f'{ticker_names[series_idx]} Actual')\n",
        "\n",
        "            # Predicted future\n",
        "            ax.plot(forecast_steps, pred[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2.5, alpha=0.8,\n",
        "                   linestyle=':', marker='s', markersize=4,\n",
        "                   label=f'{ticker_names[series_idx]} Predicted')\n",
        "\n",
        "        # Forecast start line\n",
        "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1.5,\n",
        "                  label='Forecast Start')\n",
        "\n",
        "        ax.set_xlabel('Time Steps', fontsize=10)\n",
        "        ax.set_ylabel('Normalized Price', fontsize=10)\n",
        "        ax.set_title(f'Test Sample {sample_idx+1}', fontsize=12, fontweight='bold')\n",
        "        ax.legend(loc='best', fontsize=7, ncol=2)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('2_time_series_forecasts.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 2_time_series_forecasts.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DETAILED SINGLE FORECAST WITH ALL ASSETS\n",
        "# ============================================================================\n",
        "print(\"3/5: Creating detailed multi-asset forecast...\")\n",
        "\n",
        "interesting_sample_idx = min(100, len(test_dataset) - 1)\n",
        "x_sample, y_sample = test_dataset[interesting_sample_idx]\n",
        "x_sample = x_sample.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "input_seq = test_dataset[interesting_sample_idx][0].numpy()\n",
        "actual_forecast = test_dataset[interesting_sample_idx][1].numpy()\n",
        "\n",
        "n_assets = min(10, input_seq.shape[0])\n",
        "fig, axes = plt.subplots(5, 2, figsize=(16, 16))\n",
        "fig.suptitle(f'Detailed Forecast - All Assets (Sample {interesting_sample_idx+1})',\n",
        "             fontsize=16, fontweight='bold')\n",
        "axes = axes.flatten()\n",
        "\n",
        "lookback_steps = range(-input_seq.shape[1], 0)\n",
        "forecast_steps = range(0, actual_forecast.shape[1])\n",
        "\n",
        "for asset_idx in range(n_assets):\n",
        "    ax = axes[asset_idx]\n",
        "\n",
        "    # Historical\n",
        "    ax.plot(lookback_steps, input_seq[asset_idx],\n",
        "           color='gray', linewidth=2, alpha=0.7, label='Historical')\n",
        "\n",
        "    # Actual\n",
        "    ax.plot(forecast_steps, actual_forecast[asset_idx],\n",
        "           color='#e74c3c', linewidth=2.5, marker='o', markersize=5,\n",
        "           linestyle='--', alpha=0.8, label='Actual')\n",
        "\n",
        "    # Predicted\n",
        "    ax.plot(forecast_steps, pred[asset_idx],\n",
        "           color='#2ecc71', linewidth=2.5, marker='s', markersize=5,\n",
        "           linestyle=':', alpha=0.8, label='Predicted')\n",
        "\n",
        "    # Forecast start\n",
        "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1.5)\n",
        "\n",
        "    # Metrics\n",
        "    mse = np.mean((pred[asset_idx] - actual_forecast[asset_idx])**2)\n",
        "    mae = np.mean(np.abs(pred[asset_idx] - actual_forecast[asset_idx]))\n",
        "\n",
        "    ax.set_xlabel('Time Steps', fontsize=9)\n",
        "    ax.set_ylabel('Normalized Price', fontsize=9)\n",
        "    ax.set_title(f'{ticker_names[asset_idx]} | MSE: {mse:.4f} | MAE: {mae:.4f}',\n",
        "                fontsize=11, fontweight='bold')\n",
        "    ax.legend(loc='best', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('3_detailed_asset_forecasts.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 3_detailed_asset_forecasts.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. FORECAST ACCURACY HEATMAP\n",
        "# ============================================================================\n",
        "print(\"4/5: Creating forecast accuracy heatmap...\")\n",
        "\n",
        "n_heatmap_samples = min(50, len(test_dataset))\n",
        "errors_matrix = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(n_heatmap_samples):\n",
        "        x_sample, y_sample = test_dataset[i]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Absolute error for each asset\n",
        "        error = np.abs(pred - y_sample.numpy())\n",
        "        errors_matrix.append(error.mean(axis=1))\n",
        "\n",
        "errors_matrix = np.array(errors_matrix).T\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "im = ax.imshow(errors_matrix, aspect='auto', cmap='RdYlGn_r', interpolation='nearest')\n",
        "\n",
        "ax.set_xticks(range(0, n_heatmap_samples, 5))\n",
        "ax.set_xticklabels(range(0, n_heatmap_samples, 5))\n",
        "ax.set_yticks(range(len(ticker_names)))\n",
        "ax.set_yticklabels(ticker_names)\n",
        "\n",
        "ax.set_xlabel('Test Sample Index', fontsize=12)\n",
        "ax.set_ylabel('Asset', fontsize=12)\n",
        "ax.set_title('Forecast Error Heatmap (MAE) - Green=Good, Red=Bad',\n",
        "            fontsize=14, fontweight='bold')\n",
        "\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Mean Absolute Error', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('4_forecast_error_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 4_forecast_error_heatmap.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. ROLLING FORECAST PERFORMANCE\n",
        "# ============================================================================\n",
        "print(\"5/5: Creating rolling forecast performance analysis...\")\n",
        "\n",
        "rolling_mse = []\n",
        "rolling_mae = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_dataset)):\n",
        "        x_sample, y_sample = test_dataset[i]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        mse = np.mean((pred - y_sample.numpy())**2)\n",
        "        mae = np.mean(np.abs(pred - y_sample.numpy()))\n",
        "\n",
        "        rolling_mse.append(mse)\n",
        "        rolling_mae.append(mae)\n",
        "\n",
        "# Rolling averages\n",
        "window_size = 20\n",
        "rolling_mse_avg = np.convolve(rolling_mse, np.ones(window_size)/window_size, mode='valid')\n",
        "rolling_mae_avg = np.convolve(rolling_mae, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "fig.suptitle('Rolling Forecast Performance Over Test Set', fontsize=16, fontweight='bold')\n",
        "\n",
        "# MSE\n",
        "axes[0].plot(rolling_mse, alpha=0.3, color='lightblue', label='Individual Samples')\n",
        "axes[0].plot(range(window_size-1, len(rolling_mse)), rolling_mse_avg,\n",
        "            linewidth=2.5, color='darkblue', label=f'{window_size}-Sample Average')\n",
        "axes[0].set_ylabel('MSE', fontsize=11)\n",
        "axes[0].set_title('Mean Squared Error', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# MAE\n",
        "axes[1].plot(rolling_mae, alpha=0.3, color='lightcoral', label='Individual Samples')\n",
        "axes[1].plot(range(window_size-1, len(rolling_mae)), rolling_mae_avg,\n",
        "            linewidth=2.5, color='darkred', label=f'{window_size}-Sample Average')\n",
        "axes[1].set_xlabel('Test Sample Index', fontsize=11)\n",
        "axes[1].set_ylabel('MAE', fontsize=11)\n",
        "axes[1].set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('5_rolling_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 5_rolling_performance.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"âœ… ALL VISUALIZATIONS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  1. 1_training_dashboard.png       - Training metrics & overfitting analysis\")\n",
        "print(\"  2. 2_time_series_forecasts.png    - 6 sample forecasts with context\")\n",
        "print(\"  3. 3_detailed_asset_forecasts.png - All 10 assets for one sample\")\n",
        "print(\"  4. 4_forecast_error_heatmap.png   - Error patterns across assets & time\")\n",
        "print(\"  5. 5_rolling_performance.png      - Performance trends over test set\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"FINAL MODEL QUALITY: {quality}\")\n",
        "print(f\"Test/Train Ratio: {test_train_ratio:.2f}x\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Get device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ“Š GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1. TRAINING METRICS DASHBOARD\n",
        "# ============================================================================\n",
        "print(\"1/5: Creating training metrics dashboard...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Training Metrics Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "epochs = range(len(train_losses))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(epochs, train_losses, label='Train Loss', linewidth=2, color='#2ecc71')\n",
        "axes[0, 0].plot(epochs, val_losses, label='Val Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].axhline(y=test_loss, color='#9b59b6', linestyle='--', linewidth=2,\n",
        "                   label=f'Test Loss ({test_loss:.3f})')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Loss (MSE)', fontsize=11)\n",
        "axes[0, 0].set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Log scale\n",
        "axes[0, 1].semilogy(epochs, train_losses, label='Train', linewidth=2, color='#2ecc71')\n",
        "axes[0, 1].semilogy(epochs, val_losses, label='Val', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Loss (Log Scale)', fontsize=11)\n",
        "axes[0, 1].set_title('Loss Trends (Log Scale)', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Overfitting gap\n",
        "overfitting_gap = [v/t if t > 0 else 0 for v, t in zip(val_losses, train_losses)]\n",
        "axes[1, 0].plot(epochs, overfitting_gap, linewidth=2, color='#e67e22')\n",
        "axes[1, 0].axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='No Overfitting (1x)')\n",
        "axes[1, 0].axhline(y=2.0, color='orange', linestyle='--', alpha=0.5, label='Acceptable (2x)')\n",
        "axes[1, 0].axhline(y=5.0, color='red', linestyle='--', alpha=0.5, label='High (5x)')\n",
        "axes[1, 0].fill_between(epochs, 0, 2, alpha=0.1, color='green')\n",
        "axes[1, 0].fill_between(epochs, 2, 5, alpha=0.1, color='orange')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Val/Train Ratio', fontsize=11)\n",
        "axes[1, 0].set_title('Overfitting Gap (Lower is Better)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_ylim([0, min(max(overfitting_gap), 20)])\n",
        "\n",
        "# Summary\n",
        "axes[1, 1].axis('off')\n",
        "test_train_ratio = test_loss / train_losses[-1]\n",
        "test_val_ratio = test_loss / val_losses[-1]\n",
        "\n",
        "if test_train_ratio < 2.0:\n",
        "    quality = \"ðŸŸ¢ EXCELLENT\"\n",
        "elif test_train_ratio < 5.0:\n",
        "    quality = \"ðŸŸ¡ GOOD\"\n",
        "elif test_train_ratio < 10.0:\n",
        "    quality = \"ðŸŸ  FAIR - Needs Improvement\"\n",
        "else:\n",
        "    quality = \"ðŸ”´ POOR - Severe Overfitting\"\n",
        "\n",
        "summary = f\"\"\"\n",
        "FINAL RESULTS\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "Train Loss:      {train_losses[-1]:.6f}\n",
        "Val Loss:        {val_losses[-1]:.6f}\n",
        "Test Loss:       {test_loss:.6f}\n",
        "\n",
        "Test/Train:      {test_train_ratio:.2f}x\n",
        "Test/Val:        {test_val_ratio:.2f}x\n",
        "Val/Train:       {val_losses[-1]/train_losses[-1]:.2f}x\n",
        "\n",
        "Epochs:          {len(train_losses)}\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "QUALITY:         {quality}\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "Target: < 2.0x for excellent\n",
        "        < 5.0x for good\n",
        "\"\"\"\n",
        "axes[1, 1].text(0.05, 0.5, summary, fontsize=11, family='monospace',\n",
        "                verticalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('1_training_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 1_training_dashboard.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. TIME SERIES FORECASTS - MULTIPLE SAMPLES\n",
        "# ============================================================================\n",
        "print(\"2/5: Creating time series forecast visualizations...\")\n",
        "\n",
        "model.eval()\n",
        "ticker_names = ['SPY', 'QQQ', 'IWM', 'DIA', 'GLD', 'TLT', 'UUP', 'EEM', 'VNQ', 'XLE']\n",
        "\n",
        "# Get predictions for 6 test samples\n",
        "n_samples = 6\n",
        "sample_indices = [0, 50, 100, 150, 200, 250]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "fig.suptitle('Time Series Forecasts: Historical Context + Predictions',\n",
        "             fontsize=16, fontweight='bold')\n",
        "axes = axes.flatten()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for plot_idx, sample_idx in enumerate(sample_indices):\n",
        "        if sample_idx >= len(test_dataset):\n",
        "            continue\n",
        "\n",
        "        # Get input and target\n",
        "        x_sample, y_sample = test_dataset[sample_idx]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get prediction\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Get actual values\n",
        "        input_seq = test_dataset[sample_idx][0].numpy()\n",
        "        actual_forecast = test_dataset[sample_idx][1].numpy()\n",
        "\n",
        "        ax = axes[plot_idx]\n",
        "\n",
        "        # Plot for first 3 assets\n",
        "        lookback_steps = range(-input_seq.shape[1], 0)\n",
        "        forecast_steps = range(0, actual_forecast.shape[1])\n",
        "\n",
        "        colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "\n",
        "        for series_idx in range(min(3, input_seq.shape[0])):\n",
        "            # Historical data\n",
        "            ax.plot(lookback_steps, input_seq[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2, alpha=0.6,\n",
        "                   label=f'{ticker_names[series_idx]} History')\n",
        "\n",
        "            # Actual future\n",
        "            ax.plot(forecast_steps, actual_forecast[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2.5, alpha=0.8,\n",
        "                   linestyle='--', marker='o', markersize=4,\n",
        "                   label=f'{ticker_names[series_idx]} Actual')\n",
        "\n",
        "            # Predicted future\n",
        "            ax.plot(forecast_steps, pred[series_idx],\n",
        "                   color=colors[series_idx], linewidth=2.5, alpha=0.8,\n",
        "                   linestyle=':', marker='s', markersize=4,\n",
        "                   label=f'{ticker_names[series_idx]} Predicted')\n",
        "\n",
        "        # Forecast start line\n",
        "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1.5,\n",
        "                  label='Forecast Start')\n",
        "\n",
        "        ax.set_xlabel('Time Steps', fontsize=10)\n",
        "        ax.set_ylabel('Normalized Price', fontsize=10)\n",
        "        ax.set_title(f'Test Sample {sample_idx+1}', fontsize=12, fontweight='bold')\n",
        "        ax.legend(loc='best', fontsize=7, ncol=2)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('2_time_series_forecasts.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 2_time_series_forecasts.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DETAILED SINGLE FORECAST WITH ALL ASSETS\n",
        "# ============================================================================\n",
        "print(\"3/5: Creating detailed multi-asset forecast...\")\n",
        "\n",
        "interesting_sample_idx = min(100, len(test_dataset) - 1)\n",
        "x_sample, y_sample = test_dataset[interesting_sample_idx]\n",
        "x_sample = x_sample.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "input_seq = test_dataset[interesting_sample_idx][0].numpy()\n",
        "actual_forecast = test_dataset[interesting_sample_idx][1].numpy()\n",
        "\n",
        "n_assets = min(10, input_seq.shape[0])\n",
        "fig, axes = plt.subplots(5, 2, figsize=(16, 16))\n",
        "fig.suptitle(f'Detailed Forecast - All Assets (Sample {interesting_sample_idx+1})',\n",
        "             fontsize=16, fontweight='bold')\n",
        "axes = axes.flatten()\n",
        "\n",
        "lookback_steps = range(-input_seq.shape[1], 0)\n",
        "forecast_steps = range(0, actual_forecast.shape[1])\n",
        "\n",
        "for asset_idx in range(n_assets):\n",
        "    ax = axes[asset_idx]\n",
        "\n",
        "    # Historical\n",
        "    ax.plot(lookback_steps, input_seq[asset_idx],\n",
        "           color='gray', linewidth=2, alpha=0.7, label='Historical')\n",
        "\n",
        "    # Actual\n",
        "    ax.plot(forecast_steps, actual_forecast[asset_idx],\n",
        "           color='#e74c3c', linewidth=2.5, marker='o', markersize=5,\n",
        "           linestyle='--', alpha=0.8, label='Actual')\n",
        "\n",
        "    # Predicted\n",
        "    ax.plot(forecast_steps, pred[asset_idx],\n",
        "           color='#2ecc71', linewidth=2.5, marker='s', markersize=5,\n",
        "           linestyle=':', alpha=0.8, label='Predicted')\n",
        "\n",
        "    # Forecast start\n",
        "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1.5)\n",
        "\n",
        "    # Metrics\n",
        "    mse = np.mean((pred[asset_idx] - actual_forecast[asset_idx])**2)\n",
        "    mae = np.mean(np.abs(pred[asset_idx] - actual_forecast[asset_idx]))\n",
        "\n",
        "    ax.set_xlabel('Time Steps', fontsize=9)\n",
        "    ax.set_ylabel('Normalized Price', fontsize=9)\n",
        "    ax.set_title(f'{ticker_names[asset_idx]} | MSE: {mse:.4f} | MAE: {mae:.4f}',\n",
        "                fontsize=11, fontweight='bold')\n",
        "    ax.legend(loc='best', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('3_detailed_asset_forecasts.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 3_detailed_asset_forecasts.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. FORECAST ACCURACY HEATMAP\n",
        "# ============================================================================\n",
        "print(\"4/5: Creating forecast accuracy heatmap...\")\n",
        "\n",
        "n_heatmap_samples = min(50, len(test_dataset))\n",
        "errors_matrix = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(n_heatmap_samples):\n",
        "        x_sample, y_sample = test_dataset[i]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Absolute error for each asset\n",
        "        error = np.abs(pred - y_sample.numpy())\n",
        "        errors_matrix.append(error.mean(axis=1))\n",
        "\n",
        "errors_matrix = np.array(errors_matrix).T\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "im = ax.imshow(errors_matrix, aspect='auto', cmap='RdYlGn_r', interpolation='nearest')\n",
        "\n",
        "ax.set_xticks(range(0, n_heatmap_samples, 5))\n",
        "ax.set_xticklabels(range(0, n_heatmap_samples, 5))\n",
        "ax.set_yticks(range(len(ticker_names)))\n",
        "ax.set_yticklabels(ticker_names)\n",
        "\n",
        "ax.set_xlabel('Test Sample Index', fontsize=12)\n",
        "ax.set_ylabel('Asset', fontsize=12)\n",
        "ax.set_title('Forecast Error Heatmap (MAE) - Green=Good, Red=Bad',\n",
        "            fontsize=14, fontweight='bold')\n",
        "\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Mean Absolute Error', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('4_forecast_error_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 4_forecast_error_heatmap.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. ROLLING FORECAST PERFORMANCE\n",
        "# ============================================================================\n",
        "print(\"5/5: Creating rolling forecast performance analysis...\")\n",
        "\n",
        "rolling_mse = []\n",
        "rolling_mae = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_dataset)):\n",
        "        x_sample, y_sample = test_dataset[i]\n",
        "        x_sample = x_sample.unsqueeze(0).to(device)\n",
        "        pred = model(x_sample).squeeze(0).cpu().numpy()\n",
        "\n",
        "        mse = np.mean((pred - y_sample.numpy())**2)\n",
        "        mae = np.mean(np.abs(pred - y_sample.numpy()))\n",
        "\n",
        "        rolling_mse.append(mse)\n",
        "        rolling_mae.append(mae)\n",
        "\n",
        "# Rolling averages\n",
        "window_size = 20\n",
        "rolling_mse_avg = np.convolve(rolling_mse, np.ones(window_size)/window_size, mode='valid')\n",
        "rolling_mae_avg = np.convolve(rolling_mae, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "fig.suptitle('Rolling Forecast Performance Over Test Set', fontsize=16, fontweight='bold')\n",
        "\n",
        "# MSE\n",
        "axes[0].plot(rolling_mse, alpha=0.3, color='lightblue', label='Individual Samples')\n",
        "axes[0].plot(range(window_size-1, len(rolling_mse)), rolling_mse_avg,\n",
        "            linewidth=2.5, color='darkblue', label=f'{window_size}-Sample Average')\n",
        "axes[0].set_ylabel('MSE', fontsize=11)\n",
        "axes[0].set_title('Mean Squared Error', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# MAE\n",
        "axes[1].plot(rolling_mae, alpha=0.3, color='lightcoral', label='Individual Samples')\n",
        "axes[1].plot(range(window_size-1, len(rolling_mae)), rolling_mae_avg,\n",
        "            linewidth=2.5, color='darkred', label=f'{window_size}-Sample Average')\n",
        "axes[1].set_xlabel('Test Sample Index', fontsize=11)\n",
        "axes[1].set_ylabel('MAE', fontsize=11)\n",
        "axes[1].set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('5_rolling_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"   âœ“ Saved: 5_rolling_performance.png\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"âœ… ALL VISUALIZATIONS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  1. 1_training_dashboard.png       - Training metrics & overfitting analysis\")\n",
        "print(\"  2. 2_time_series_forecasts.png    - 6 sample forecasts with context\")\n",
        "print(\"  3. 3_detailed_asset_forecasts.png - All 10 assets for one sample\")\n",
        "print(\"  4. 4_forecast_error_heatmap.png   - Error patterns across assets & time\")\n",
        "print(\"  5. 5_rolling_performance.png      - Performance trends over test set\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"FINAL MODEL QUALITY: {quality}\")\n",
        "print(f\"Test/Train Ratio: {test_train_ratio:.2f}x\")\n",
        "print(f\"{'='*80}\\n\")"
      ],
      "metadata": {
        "id": "_pEC-wvYTYig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}